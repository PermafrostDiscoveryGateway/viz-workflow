{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsl Worflow Breakdown\n",
    "\n",
    "Working through the parsl workflow for PDG datasets in chunks. Sample dataset is for lake change in the Arctic, provided by Ingmar Nitze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "# file paths\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# visualization\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import box\n",
    "\n",
    "# PDG packages\n",
    "import pdgstaging\n",
    "import pdgraster\n",
    "import py3dtiles\n",
    "import viz_3dtiles\n",
    "#from viz_3dtiles import TreeGenerator, BoundingVolumeRegion\n",
    "#import pdgpy3dtiles\n",
    "#from StagedTo3DConverter import StagedTo3DConverter\n",
    "\n",
    "# logging and configuration\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import logging.config\n",
    "import argparse\n",
    "import json\n",
    "\n",
    "# Parsl\n",
    "import parsl\n",
    "from parsl import python_app\n",
    "from parsl.config import Config\n",
    "#from parsl.channels import LocalChannel\n",
    "from parsl.executors import HighThroughputExecutor\n",
    "from parsl.providers import LocalProvider\n",
    "#from parsl.executors.threads import ThreadPoolExecutor\n",
    "#from parsl.providers import LocalProvider\n",
    "#from parsl.providers import KubernetesProvider\n",
    "#from parsl.addresses import address_by_route\n",
    "#from kubernetes import client, config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set configuration and data path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_config = '/home/jcohen/viz-workflow/workflow_configs/ingmar-config.json'\n",
    "logging_config = '/home/jcohen/viz-workflow/workflow_configs/logging.json'\n",
    "base_dir = Path('/home/jcohen/gpkg_files_expanded')\n",
    "filename = 'lake_change_*.gpkg'\n",
    "input = [p.as_posix() for p in base_dir.glob('**/' + filename)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/jcohen/gpkg_files_expanded/lake_change_32601.gpkg',\n",
       " '/home/jcohen/gpkg_files_expanded/lake_change_32602.gpkg',\n",
       " '/home/jcohen/gpkg_files_expanded/lake_change_32603.gpkg',\n",
       " '/home/jcohen/gpkg_files_expanded/lake_change_32604.gpkg',\n",
       " '/home/jcohen/gpkg_files_expanded/lake_change_32605.gpkg',\n",
       " '/home/jcohen/gpkg_files_expanded/lake_change_32606.gpkg',\n",
       " '/home/jcohen/gpkg_files_expanded/lake_change_32607.gpkg',\n",
       " '/home/jcohen/gpkg_files_expanded/lake_change_32608.gpkg',\n",
       " '/home/jcohen/gpkg_files_expanded/lake_change_32609.gpkg',\n",
       " '/home/jcohen/gpkg_files_expanded/lake_change_32610.gpkg']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# workflow_config = '/home/jcohen/viz-workflow/workflow_configs/ingmar-config.json'\n",
    "# logging_config = '/home/jcohen/viz-workflow/workflow_configs/logging.json'\n",
    "\n",
    "# base_dir = Path('/home/pdg/data/nitze_lake_change/data_sample_2022-09-09')\n",
    "# subdirs = ['32607', '32608', '32609']\n",
    "# filename = 'lake_change.gpkg'\n",
    "# # to define each .gpkg file within each UTM subdir as a string representation with forward slashes, use as_posix() for each iteration\n",
    "# # of base_dir + filename. The ** represents that any subdir string can be present between the base_dir and the filename, meaning I do not\n",
    "# # think that we needed to create the object subdirs above\n",
    "# input = [p.as_posix() for p in base_dir.glob('**/' + filename)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up `parsl` HighThroughputExecutor\n",
    "\n",
    "This will configure how we distribute the parallelization across our workers for staging, rasterizing, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<parsl.dataflow.dflow.DataFlowKernel at 0x7f771ef12260>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# skeleton code copied from ADC Scalable Computing Workshop\n",
    "\n",
    "# bash command to activate virtual environment\n",
    "activate_env = 'conda activate pdgviz'\n",
    "\n",
    "htex_config_local = Config(\n",
    "  executors = [\n",
    "      HighThroughputExecutor(\n",
    "          # label argument not necessary bc not using kubernetes\n",
    "          # cores_per_worker = 1 # not sure if this is necesary \n",
    "          # max_workers in parsl-workflow = 2...why would it be so low? because just testing? \n",
    "        max_workers = 32,\n",
    "          # worker_logdir_root = '/' only necessary if the file system is remote, which is not the case for this lake change sample\n",
    "          # address not necessary because we are not using kubernetes\n",
    "        worker_debug = True, # helps debugging\n",
    "          # provider is local for this run thru, kubernetes would use KubernetesProvider()\n",
    "        provider = LocalProvider(\n",
    "          #channel = LocalChannel(), # not sure what this does, might be default?\n",
    "          worker_init = activate_env,\n",
    "          init_blocks = 1, # default I think\n",
    "          max_blocks = 1 # default\n",
    "          ),\n",
    "      )\n",
    "  ],\n",
    "  strategy = None # we are not using a scheduler for compute nodes so we don't want a scaling strategy\n",
    ")\n",
    "\n",
    "parsl.clear() # first clear the current configuration since we will likely run this script multiple times\n",
    "parsl.load(htex_config_local) # load the config we just outlined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, [here](https://github.com/PermafrostDiscoveryGateway/viz-workflow/blob/0beb3b14239f2dd8cd4329026dc8d9a41aece7d7/pdg_workflow/pdg_workflow.py#L32) is the HighThroughputExecutor used in the `parsl-workflow` branch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explicitly define StagedTo3DConverter class & its methods rather than sourcing it in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class StagedTo3DConverter():\n",
    "    \"\"\"\n",
    "        Processes staged vector data into Cesium 3D tiles according to the\n",
    "        settings in a config file or dict. This class acts as the orchestrator\n",
    "        of the other viz-3dtiles classes, and coordinates the sending and\n",
    "        receiving of information between them.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config\n",
    "    ):\n",
    "        \"\"\"\n",
    "            Initialize the StagedTo3DConverter class.\n",
    "            Parameters\n",
    "            ----------\n",
    "            config : dict or str\n",
    "                A dictionary of configuration settings or a path to a config\n",
    "                JSON file. (See help(pdgstaging.ConfigManager))\n",
    "        \"\"\"\n",
    "\n",
    "        self.config = pdgstaging.ConfigManager(config)\n",
    "        self.tiles = pdgstaging.TilePathManager(\n",
    "            **self.config.get_path_manager_config())\n",
    "\n",
    "    def all_staged_to_3dtiles(\n",
    "        self\n",
    "    ):\n",
    "        \"\"\"\n",
    "            Process all staged vector tiles into 3D tiles.\n",
    "        \"\"\"\n",
    "\n",
    "        # Get the list of staged vector tiles\n",
    "        paths = self.tiles.get_filenames_from_dir('staged')\n",
    "        # Process each tile\n",
    "        for path in paths:\n",
    "            self.staged_to_3dtile(path)\n",
    "\n",
    "    def staged_to_3dtile(self, path):\n",
    "        \"\"\"\n",
    "            Convert a staged vector tile into a B3DM tile file and a matching\n",
    "            JSON tileset file.\n",
    "            Parameters\n",
    "            ----------\n",
    "            path : str\n",
    "                The path to the staged vector tile.\n",
    "            Returns\n",
    "            -------\n",
    "            tile, tileset : Cesium3DTile, Tileset\n",
    "                The Cesium3DTiles and Cesium3DTileset objects\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "\n",
    "            # Get information about the tile from the path\n",
    "            tile = self.tiles.tile_from_path(path)\n",
    "            out_path = self.tiles.path_from_tile(tile, '3dtiles')\n",
    "\n",
    "            tile_bv = self.bounding_region_for_tile(tile)\n",
    "\n",
    "            # Get the filename of the tile WITHOUT the extension\n",
    "            tile_filename = os.path.splitext(os.path.basename(out_path))[0]\n",
    "            # Get the base of the path, without the filename\n",
    "            tile_dir = os.path.dirname(out_path) + os.path.sep\n",
    "\n",
    "            # Log the event\n",
    "            logger.info(\n",
    "                f'Creating 3dtile from {path} for tile {tile} to {out_path}.')\n",
    "\n",
    "            # Read in the staged vector tile\n",
    "            gdf = gpd.read_file(path)\n",
    "\n",
    "            # Check if the gdf is empty\n",
    "            if len(gdf) == 0:\n",
    "                logger.warning(\n",
    "                    f'Vector tile {path} is empty. 3D tile will not be'\n",
    "                    ' created.')\n",
    "                return\n",
    "\n",
    "            # Remove polygons with centroids that are outside the tile boundary\n",
    "            prop_cent_in_tile = self.config.polygon_prop(\n",
    "                'centroid_within_tile')\n",
    "            gdf = gdf[gdf[prop_cent_in_tile]]\n",
    "\n",
    "            # Check if deduplication should be performed\n",
    "            dedup_here = self.config.deduplicate_at('3dtiles')\n",
    "            dedup_method = self.config.get_deduplication_method()\n",
    "\n",
    "            # Deduplicate if required\n",
    "            if dedup_here and (dedup_method is not None):\n",
    "                dedup_config = self.config.get_deduplication_config(gdf)\n",
    "                dedup = dedup_method(gdf, **dedup_config)\n",
    "                gdf = dedup['keep']\n",
    "\n",
    "                # The tile could theoretically be empty after deduplication\n",
    "                if len(gdf) == 0:\n",
    "                    logger.warning(\n",
    "                        f'Vector tile {path} is empty after deduplication.'\n",
    "                        ' 3D Tile will not be created.')\n",
    "                    return\n",
    "\n",
    "            # Create & save the b3dm file\n",
    "            ces_tile, ces_tileset = TreeGenerator.leaf_tile_from_gdf(\n",
    "                gdf,\n",
    "                dir=tile_dir,\n",
    "                filename=tile_filename,\n",
    "                z=self.config.get('z_coord'),\n",
    "                geometricError=self.config.get('geometricError'),\n",
    "                tilesetVersion=self.config.get('version'),\n",
    "                boundingVolume=tile_bv\n",
    "            )\n",
    "\n",
    "            return ces_tile, ces_tileset\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f'Error creating 3D Tile from {path}.')\n",
    "            logger.error(e)\n",
    "\n",
    "    def parent_3dtiles_from_children(self, tiles, bv_limit=None):\n",
    "        \"\"\"\n",
    "            Create parent Cesium 3D Tileset json files that point to of child\n",
    "            JSON files in the tile tree hierarchy. This method will take a list\n",
    "            of parent tiles and search the 3D tile directory for any children\n",
    "            tiles to create.\n",
    "            Parameters\n",
    "            ----------\n",
    "            tiles : list of morecantile.Tile\n",
    "                The list of parent tiles to create.\n",
    "        \"\"\"\n",
    "\n",
    "        tile_manager = self.tiles\n",
    "        config_manager = self.config\n",
    "\n",
    "        tileset_objs = []\n",
    "\n",
    "        # Make the next level of parent tiles\n",
    "        for parent_tile in tiles:\n",
    "            # Get the path to the parent tile\n",
    "            parent_path = tile_manager.path_from_tile(parent_tile, '3dtiles')\n",
    "            # Get just the base dir without the filename\n",
    "            parent_dir = os.path.dirname(parent_path)\n",
    "            # Get the filename of the parent tile, without the extension\n",
    "            parent_filename = os.path.basename(parent_path)\n",
    "            parent_filename = os.path.splitext(parent_filename)[0]\n",
    "            # Get the children paths for this parent tile\n",
    "            child_paths = tile_manager.get_child_paths(parent_tile, '3dtiles')\n",
    "            # Remove paths that do not exist\n",
    "            child_paths = tile_manager.remove_nonexistent_paths(child_paths)\n",
    "            # Get the parent bounding volume\n",
    "            parent_bv = self.bounding_region_for_tile(\n",
    "                parent_tile, limit_to=bv_limit)\n",
    "            # If the bounding region is outside t\n",
    "            # Get the version\n",
    "            version = config_manager.get('version')\n",
    "            # Get the geometric error\n",
    "            geometric_error = config_manager.get('geometricError')\n",
    "            # Create the parent tile\n",
    "            tileset_obj = TreeGenerator.parent_tile_from_children_json(\n",
    "                child_paths,\n",
    "                dir=parent_dir,\n",
    "                filename=parent_filename,\n",
    "                geometricError=geometric_error,\n",
    "                tilesetVersion=version,\n",
    "                boundingVolume=parent_bv\n",
    "            )\n",
    "            tileset_objs.append(tileset_obj)\n",
    "\n",
    "        return tileset_objs\n",
    "\n",
    "    def bounding_region_for_tile(self, tile, limit_to=None):\n",
    "        \"\"\"\n",
    "        For a morecantile.Tile object, return a BoundingVolumeRegion object\n",
    "        that represents the bounding region of the tile.\n",
    "        Parameters\n",
    "        ----------\n",
    "        tile : morecantile.Tile\n",
    "            The tile object.\n",
    "        limit_to : list of float\n",
    "            Optional list of west, south, east, north coordinates to limit\n",
    "            the bounding region to.\n",
    "        Returns\n",
    "        -------\n",
    "        bv : BoundingVolumeRegion\n",
    "            The bounding region object.\n",
    "        \"\"\"\n",
    "        tms = self.tiles.tms\n",
    "        bounds = tms.bounds(tile)\n",
    "        bounds = gpd.GeoSeries(\n",
    "            box(bounds.left, bounds.bottom, bounds.right, bounds.top),\n",
    "            crs=tms.crs)\n",
    "        if limit_to is not None:\n",
    "            bounds_limitor = gpd.GeoSeries(\n",
    "                box(limit_to[0], limit_to[1], limit_to[2], limit_to[3]),\n",
    "                crs=tms.crs)\n",
    "            bounds = bounds.intersection(bounds_limitor)\n",
    "        bounds = bounds.to_crs(BoundingVolumeRegion.CESIUM_EPSG)\n",
    "        bounds = bounds.total_bounds\n",
    "\n",
    "        region_bv = {\n",
    "            'west': bounds[0], 'south': bounds[1],\n",
    "            'east': bounds[2], 'north': bounds[3],\n",
    "        }\n",
    "        return region_bv\n",
    "\n",
    "    def make_top_level_tileset(self):\n",
    "        \"\"\"\n",
    "        Create a top-level tileset.json file that sets all the min_z level\n",
    "        tiles as its children. This is needed to display the tiles in Cesium\n",
    "        when the min_z level has more than one tile.\n",
    "        Returns\n",
    "        -------\n",
    "        tileset : Tileset\n",
    "            The Cesium3DTileset object\n",
    "        \"\"\"\n",
    "\n",
    "        tile_manager = self.tiles\n",
    "        config_manager = self.config\n",
    "        min_z = config_manager.get_min_z()\n",
    "\n",
    "        # Make a parent tileset.json - this will combine the top level tiles if\n",
    "        # there are 2, otherwise it will just refer to the top level tile.\n",
    "        top_level_tiles = tile_manager.get_filenames_from_dir(\n",
    "            '3dtiles', z=min_z)\n",
    "        top_level_dir = tile_manager.get_base_dir('3dtiles')['path']\n",
    "\n",
    "        return TreeGenerator.parent_tile_from_children_json(\n",
    "            children=top_level_tiles,\n",
    "            dir=top_level_dir\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logging(log_json_file):\n",
    "    \"\"\"\n",
    "    Setup logging configuration\n",
    "    \"\"\"\n",
    "    with open(log_json_file, 'r') as f:\n",
    "        logging_dict = json.load(f)\n",
    "    logging.config.dictConfig(logging_dict)\n",
    "    return logging_dict\n",
    "\n",
    "logging_dict = setup_logging(logging_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define batch sizes and batching function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_staging=2 \n",
    "batch_size_rasterization=30\n",
    "batch_size_3dtiles=20\n",
    "batch_size_parent_3dtiles=500\n",
    "batch_size_geotiffs=200\n",
    "batch_size_web_tiles=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(items, batch_size):\n",
    "    \"\"\"\n",
    "    Create batches of a given size from a list of items.\n",
    "    \"\"\"\n",
    "    return [items[i:i + batch_size] for i in range(0, len(items), batch_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create batches of input files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk written when using original 3 gpkg files\n",
    "#input\n",
    "# this already is the paths to the input files, it is not a base dir\n",
    "# so we do not have to use stager.tiles.get_filenames_from_dir('input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['/home/jcohen/gpkg_files_expanded/lake_change_32601.gpkg',\n",
       "  '/home/jcohen/gpkg_files_expanded/lake_change_32602.gpkg'],\n",
       " ['/home/jcohen/gpkg_files_expanded/lake_change_32603.gpkg',\n",
       "  '/home/jcohen/gpkg_files_expanded/lake_change_32604.gpkg'],\n",
       " ['/home/jcohen/gpkg_files_expanded/lake_change_32605.gpkg',\n",
       "  '/home/jcohen/gpkg_files_expanded/lake_change_32606.gpkg'],\n",
       " ['/home/jcohen/gpkg_files_expanded/lake_change_32607.gpkg',\n",
       "  '/home/jcohen/gpkg_files_expanded/lake_change_32608.gpkg'],\n",
       " ['/home/jcohen/gpkg_files_expanded/lake_change_32609.gpkg',\n",
       "  '/home/jcohen/gpkg_files_expanded/lake_change_32610.gpkg']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch size = 2\n",
    "input_batches = make_batch(input, batch_size_staging)\n",
    "input_batches # 5 batches, 2 files each\n",
    "\n",
    "\n",
    "# when batch size is 1, input_batches = input\n",
    "# so we do not need to use batches for staging step?\n",
    "# move forward with just input object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the stager, raster tiler, and 3d tiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# staging configuration\n",
    "stager = pdgstaging.TileStager(workflow_config)\n",
    "tile_manager = stager.tiles\n",
    "config_manager = stager.config\n",
    "\n",
    "# zoom levels configuration\n",
    "min_z = config_manager.get_min_z()\n",
    "max_z = config_manager.get_max_z()\n",
    "parent_zs = range(max_z - 1, min_z - 1, -1)\n",
    "\n",
    "# 3D tiler configuration\n",
    "tiles3dmaker = StagedTo3DConverter(workflow_config)\n",
    "\n",
    "# raster tilerconfiguration \n",
    "rasterizer = pdgraster.RasterTiler(workflow_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up parsl app to stage in parallel\n",
    "\n",
    "Need to import all necessary packages for the staging step within parsl app for staging.\n",
    "\n",
    "I am actually not going to stage in parallel for this run thru, because there are so few input files, it messes with batching because there is only 1 gpkg per batch. I will batch for rasterization and web tiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage in parallel\n"
     ]
    }
   ],
   "source": [
    "# Decorators seem to be ignored as the first line of a cell, so print something first\n",
    "print(\"Stage in parallel\")\n",
    "\n",
    "@python_app\n",
    "def stage(paths, config, logging_dict=None):\n",
    "    \"\"\"\n",
    "    Stage files (step 1)\n",
    "    \"\"\"\n",
    "    import pdgstaging\n",
    "    if logging_dict:\n",
    "        import logging.config\n",
    "        logging.config.dictConfig(logging_dict)\n",
    "    stager = pdgstaging.TileStager(config)\n",
    "    for path in paths:\n",
    "        stager.stage(path)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage input files in parallel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/jcohen/viz-workflow/parsl_breakdown.ipynb Cell 25\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdatateam.nceas.ucsb.edu/home/jcohen/viz-workflow/parsl_breakdown.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     app_futures\u001b[39m.\u001b[39mappend(app_future)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdatateam.nceas.ucsb.edu/home/jcohen/viz-workflow/parsl_breakdown.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m#Don't continue to step 2 until all files have been staged\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bdatateam.nceas.ucsb.edu/home/jcohen/viz-workflow/parsl_breakdown.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m [a\u001b[39m.\u001b[39mresult() \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m app_futures]\n",
      "\u001b[1;32m/home/jcohen/viz-workflow/parsl_breakdown.ipynb Cell 25\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdatateam.nceas.ucsb.edu/home/jcohen/viz-workflow/parsl_breakdown.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     app_futures\u001b[39m.\u001b[39mappend(app_future)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdatateam.nceas.ucsb.edu/home/jcohen/viz-workflow/parsl_breakdown.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m#Don't continue to step 2 until all files have been staged\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bdatateam.nceas.ucsb.edu/home/jcohen/viz-workflow/parsl_breakdown.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m [a\u001b[39m.\u001b[39;49mresult() \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m app_futures]\n",
      "File \u001b[0;32m~/anaconda3/envs/pdgviz/lib/python3.10/concurrent/futures/_base.py:458\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    457\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[0;32m--> 458\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[1;32m    459\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    460\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m~/anaconda3/envs/pdgviz/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pdgviz/lib/python3.10/site-packages/parsl/dataflow/dflow.py:288\u001b[0m, in \u001b[0;36mDataFlowKernel.handle_exec_update\u001b[0;34m(self, task_record, future)\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdone callback called, despite future not reporting itself as done\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    287\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 288\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_unwrap_remote_exception_wrapper(future)\n\u001b[1;32m    290\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    291\u001b[0m     logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mTask \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m try \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m failed\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(task_id, task_record[\u001b[39m'\u001b[39m\u001b[39mtry_id\u001b[39m\u001b[39m'\u001b[39m]))\n",
      "File \u001b[0;32m~/anaconda3/envs/pdgviz/lib/python3.10/site-packages/parsl/dataflow/dflow.py:483\u001b[0m, in \u001b[0;36mDataFlowKernel._unwrap_remote_exception_wrapper\u001b[0;34m(future)\u001b[0m\n\u001b[1;32m    481\u001b[0m result \u001b[39m=\u001b[39m future\u001b[39m.\u001b[39mresult()\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(result, RemoteExceptionWrapper):\n\u001b[0;32m--> 483\u001b[0m     result\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m    484\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/pdgviz/lib/python3.10/site-packages/parsl/app/errors.py:137\u001b[0m, in \u001b[0;36mRemoteExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mReraising exception of type \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(t))\n\u001b[1;32m    135\u001b[0m v \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_exception()\n\u001b[0;32m--> 137\u001b[0m reraise(t, v, v\u001b[39m.\u001b[39;49m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/pdgviz/lib/python3.10/site-packages/six.py:719\u001b[0m, in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    717\u001b[0m     \u001b[39mif\u001b[39;00m value\u001b[39m.\u001b[39m__traceback__ \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m tb:\n\u001b[1;32m    718\u001b[0m         \u001b[39mraise\u001b[39;00m value\u001b[39m.\u001b[39mwith_traceback(tb)\n\u001b[0;32m--> 719\u001b[0m     \u001b[39mraise\u001b[39;00m value\n\u001b[1;32m    720\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    721\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pdgviz/lib/python3.10/site-packages/parsl/app/errors.py:176\u001b[0m, in \u001b[0;36mwrapper\u001b[0;34m()\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    175\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[0;32m--> 176\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[39mreturn\u001b[39;00m RemoteExceptionWrapper(\u001b[39m*\u001b[39msys\u001b[39m.\u001b[39mexc_info())\n",
      "\u001b[1;32m/home/jcohen/viz-workflow/parsl_breakdown.ipynb Cell 25\u001b[0m in \u001b[0;36mstage\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdatateam.nceas.ucsb.edu/home/jcohen/viz-workflow/parsl_breakdown.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mfor\u001b[39;00m path \u001b[39min\u001b[39;00m paths:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdatateam.nceas.ucsb.edu/home/jcohen/viz-workflow/parsl_breakdown.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     stager\u001b[39m.\u001b[39mstage(path)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bdatateam.nceas.ucsb.edu/home/jcohen/viz-workflow/parsl_breakdown.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pdgviz/lib/python3.10/site-packages/pdgstaging/TileStager.py:132\u001b[0m, in \u001b[0;36mstage\u001b[0;34m()\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgrid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_tms_grid(gdf)\n\u001b[1;32m    131\u001b[0m     gdf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_properties(gdf, path)\n\u001b[0;32m--> 132\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_tiles(gdf)\n\u001b[1;32m    133\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    134\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mNo features in \u001b[39m\u001b[39m{\u001b[39;00mpath\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pdgviz/lib/python3.10/site-packages/pdgstaging/TileStager.py:420\u001b[0m, in \u001b[0;36msave_tiles\u001b[0;34m()\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[39m# Record what was saved\u001b[39;00m\n\u001b[1;32m    419\u001b[0m     data[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprops[\u001b[39m'\u001b[39m\u001b[39mtile\u001b[39m\u001b[39m'\u001b[39m]] \u001b[39m=\u001b[39m tiles\n\u001b[0;32m--> 420\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msummarize(data)\n\u001b[1;32m    421\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    422\u001b[0m     \u001b[39m# Track the end time, the total time, and the number of vectors\u001b[39;00m\n\u001b[1;32m    423\u001b[0m     logger\u001b[39m.\u001b[39minfo(\n\u001b[1;32m    424\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mSaved \u001b[39m\u001b[39m{\u001b[39;00mtile_path\u001b[39m}\u001b[39;00m\u001b[39m in \u001b[39m\u001b[39m{\u001b[39;00mdatetime\u001b[39m.\u001b[39mnow() \u001b[39m-\u001b[39m start_time\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m    425\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/pdgviz/lib/python3.10/site-packages/pdgstaging/TileStager.py:551\u001b[0m, in \u001b[0;36msummarize\u001b[0;34m()\u001b[0m\n\u001b[1;32m    544\u001b[0m gdf_summary\u001b[39m.\u001b[39mto_csv(summary_path, mode\u001b[39m=\u001b[39mmode,\n\u001b[1;32m    545\u001b[0m                    index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, header\u001b[39m=\u001b[39mheader)\n\u001b[1;32m    547\u001b[0m \u001b[39m# Log the total time to create the summary\u001b[39;00m\n\u001b[1;32m    548\u001b[0m logger\u001b[39m.\u001b[39minfo(\n\u001b[1;32m    549\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mSummarized Tile(z=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, x=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, y=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) in \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m    550\u001b[0m     \u001b[39m.\u001b[39mformat(\n\u001b[0;32m--> 551\u001b[0m         tile_props[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mtile_z\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m    552\u001b[0m         tile_props[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mtile_x\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m    553\u001b[0m         tile_props[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mtile_y\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m    554\u001b[0m         (datetime\u001b[39m.\u001b[39mnow() \u001b[39m-\u001b[39m start_time)\n\u001b[1;32m    555\u001b[0m     )\n\u001b[1;32m    556\u001b[0m )\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "app_futures = []\n",
    "for batch in input_batches:\n",
    "    app_future = stage(batch, workflow_config, logging_dict)\n",
    "    #print(app_future)\n",
    "    app_futures.append(app_future)\n",
    "\n",
    "#Don't continue to step 2 until all files have been staged\n",
    "[a.result() for a in app_futures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "htex_config_local.executors[0].shutdown()\n",
    "parsl.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch staged filepaths\n",
    "\n",
    "Now the staged file dir is complete. Moving onto preparing the staged files for rasterization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get paths to all the newly staged tiles\n",
    "staged_paths = stager.tiles.get_filenames_from_dir('staged')\n",
    "staged_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many staged files, is the batch size 30 reasonable? 19088... sure 30 sounds fine\n",
    "len(staged_paths) # matches the terminal count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch staged files\n",
    "staged_batches = make_batch(staged_paths, batch_size_rasterization)\n",
    "len(staged_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see what is within 1 batch\n",
    "staged_batches[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define parsl function to rasterize in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decorators seem to be ignored as the first line of a cell, so print something first\n",
    "print(\"Rasterize in parallel\")\n",
    "\n",
    "@python_app\n",
    "def rasterize(staged_paths, config, logging_dict=None):\n",
    "    \"\"\"\n",
    "    Rasterize a batch of vector files (step 2)\n",
    "    \"\"\"\n",
    "    import pdgraster\n",
    "    if logging_dict:\n",
    "        import logging.config\n",
    "        logging.config.dictConfig(logging_dict)\n",
    "    rasterizer = pdgraster.RasterTiler(config)\n",
    "    return rasterizer.rasterize_vectors(staged_paths, make_parents=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rasterize all staged tiles (only highest z-level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_futures = []\n",
    "for batch in staged_batches:\n",
    "    app_future = rasterize(batch, workflow_config, logging_dict)\n",
    "    app_futures.append(app_future)\n",
    "\n",
    "# Don't continue to step 3 until all tiles have been rasterized\n",
    "#[a.result() for a in app_futures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "htex_config_local.executors[0].shutdown()\n",
    "parsl.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure we have the same number of GeoTIFF files as we do staged vector tiles\n",
    "geotiff_paths = rasterizer.tiles.get_filenames_from_dir('geotiff')\n",
    "geotiff_paths\n",
    "#len(geotiff_paths) == len(staged_paths)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('pdgviz')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b54da58c51b30b82a7fc0059a0ef455812aa5d7a05176ea08a9c6d431b69c979"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
