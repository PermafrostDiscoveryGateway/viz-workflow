{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsl Worflow Breakdown\n",
    "\n",
    "Working through the parsl workflow for PDG datasets in chunks. Sample dataset is for lake change in the Arctic, provided by Ingmar Nitze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "# file paths\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# visualization\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import box\n",
    "\n",
    "# PDG packages\n",
    "import pdgstaging\n",
    "import pdgraster\n",
    "import py3dtiles\n",
    "import viz_3dtiles\n",
    "from viz_3dtiles import TreeGenerator, BoundingVolumeRegion\n",
    "from viz_3dtiles import leaf_tile_from_gdf, parent_tile_from_children_json\n",
    "#import pdgpy3dtiles\n",
    "#from StagedTo3DConverter import StagedTo3DConverter\n",
    "\n",
    "# logging and configuration\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import logging.config\n",
    "import argparse\n",
    "import json\n",
    "\n",
    "# Parsl\n",
    "import parsl\n",
    "from parsl import python_app\n",
    "from parsl.config import Config\n",
    "from parsl.channels import LocalChannel\n",
    "from parsl.executors import HighThroughputExecutor\n",
    "from parsl.providers import LocalProvider\n",
    "#from parsl.executors.threads import ThreadPoolExecutor\n",
    "#from parsl.providers import LocalProvider\n",
    "#from parsl.providers import KubernetesProvider\n",
    "#from parsl.addresses import address_by_route\n",
    "#from kubernetes import client, config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set configuration and data path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# newer & larger data sample downloaded from google drive\n",
    "\n",
    "# workflow_config = '/home/jcohen/viz-workflow/workflow_configs/ingmar-config.json'\n",
    "# logging_config = '/home/jcohen/viz-workflow/workflow_configs/logging.json'\n",
    "# base_dir = Path('/home/jcohen/gpkg_files_expanded')\n",
    "# filename = 'lake_change_*.gpkg'\n",
    "# input = [p.as_posix() for p in base_dir.glob('**/' + filename)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#workflow_config = '/home/jcohen/viz-workflow/workflow_configs/ingmar-config.json'\n",
    "workflow_config = '/home/jcohen/viz-workflow/workflow_configs/ingmar-config__updated.json'\n",
    "logging_config = '/home/jcohen/viz-workflow/workflow_configs/logging.json'\n",
    "\n",
    "base_dir = Path('/home/pdg/data/nitze_lake_change/data_sample_2022-09-09')\n",
    "subdirs = ['32607', '32608', '32609']\n",
    "filename = 'lake_change.gpkg'\n",
    "# to define each .gpkg file within each UTM subdir as a string representation with forward slashes, use as_posix() for each iteration\n",
    "# of base_dir + filename. The ** represents that any subdir string can be present between the base_dir and the filename, meaning I do not\n",
    "# think that we needed to create the object subdirs above\n",
    "input = [p.as_posix() for p in base_dir.glob('**/' + filename)]\n",
    "input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up `parsl` HighThroughputExecutor with LocalProvider\n",
    "\n",
    "This will configure how we distribute the parallelization across our workers for staging, rasterizing, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skeleton code copied from ADC Scalable Computing Workshop\n",
    "\n",
    "# bash command to activate virtual environment\n",
    "activate_env = 'source /home/jcohen/.bashrc; conda activate pdgviz'\n",
    "\n",
    "htex_config_local = Config(\n",
    "  executors = [\n",
    "      HighThroughputExecutor(\n",
    "        label = \"htex_Local\",\n",
    "        cores_per_worker = 2, \n",
    "        max_workers = 2, # why would this be so low? because just testing with small amount of data ?\n",
    "          # worker_logdir_root = '/' only necessary if the file system is remote, which is not the case for this lake change sample\n",
    "          # address not necessary because we are not using kubernetes\n",
    "        worker_debug = False, # don't need this because we have logging setup\n",
    "          # provider is local for this run thru, kubernetes would use KubernetesProvider()\n",
    "        provider = LocalProvider(\n",
    "          channel = LocalChannel(),\n",
    "          worker_init = activate_env,\n",
    "          init_blocks = 1, # default I think\n",
    "          max_blocks = 10 # changed from deafult of 1\n",
    "        ),\n",
    "      )\n",
    "    ],\n",
    "  )\n",
    "\n",
    "parsl.clear() # first clear the current configuration since we will likely run this script multiple times\n",
    "parsl.load(htex_config_local) # load the config we just outlined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, [here](https://github.com/PermafrostDiscoveryGateway/viz-workflow/blob/0beb3b14239f2dd8cd4329026dc8d9a41aece7d7/pdg_workflow/pdg_workflow.py#L32) is the HighThroughputExecutor used in the `parsl-workflow` branch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explicitly define StagedTo3DConverter class & its methods rather than sourcing it in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class StagedTo3DConverter():\n",
    "    \"\"\"\n",
    "        Processes staged vector data into Cesium 3D tiles according to the\n",
    "        settings in a config file or dict. This class acts as the orchestrator\n",
    "        of the other viz-3dtiles classes, and coordinates the sending and\n",
    "        receiving of information between them.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config\n",
    "    ):\n",
    "        \"\"\"\n",
    "            Initialize the StagedTo3DConverter class.\n",
    "            Parameters\n",
    "            ----------\n",
    "            config : dict or str\n",
    "                A dictionary of configuration settings or a path to a config\n",
    "                JSON file. (See help(pdgstaging.ConfigManager))\n",
    "        \"\"\"\n",
    "\n",
    "        self.config = pdgstaging.ConfigManager(config)\n",
    "        self.tiles = pdgstaging.TilePathManager(\n",
    "            **self.config.get_path_manager_config())\n",
    "\n",
    "    def all_staged_to_3dtiles(\n",
    "        self\n",
    "    ):\n",
    "        \"\"\"\n",
    "            Process all staged vector tiles into 3D tiles.\n",
    "        \"\"\"\n",
    "\n",
    "        # Get the list of staged vector tiles\n",
    "        paths = self.tiles.get_filenames_from_dir('staged')\n",
    "        # Process each tile\n",
    "        for path in paths:\n",
    "            self.staged_to_3dtile(path)\n",
    "\n",
    "    def staged_to_3dtile(self, path):\n",
    "        \"\"\"\n",
    "            Convert a staged vector tile into a B3DM tile file and a matching\n",
    "            JSON tileset file.\n",
    "            Parameters\n",
    "            ----------\n",
    "            path : str\n",
    "                The path to the staged vector tile.\n",
    "            Returns\n",
    "            -------\n",
    "            tile, tileset : Cesium3DTile, Tileset\n",
    "                The Cesium3DTiles and Cesium3DTileset objects\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "\n",
    "            # Get information about the tile from the path\n",
    "            tile = self.tiles.tile_from_path(path)\n",
    "            out_path = self.tiles.path_from_tile(tile, '3dtiles')\n",
    "\n",
    "            tile_bv = self.bounding_region_for_tile(tile)\n",
    "\n",
    "            # Get the filename of the tile WITHOUT the extension\n",
    "            tile_filename = os.path.splitext(os.path.basename(out_path))[0]\n",
    "            # Get the base of the path, without the filename\n",
    "            tile_dir = os.path.dirname(out_path) + os.path.sep\n",
    "\n",
    "            # Log the event\n",
    "            logger.info(\n",
    "                f'Creating 3dtile from {path} for tile {tile} to {out_path}.')\n",
    "\n",
    "            # Read in the staged vector tile\n",
    "            gdf = gpd.read_file(path)\n",
    "\n",
    "            print('finished reading in path as gdf')\n",
    "\n",
    "            # Check if the gdf is empty\n",
    "            if len(gdf) == 0:\n",
    "                logger.warning(\n",
    "                    f'Vector tile {path} is empty. 3D tile will not be'\n",
    "                    ' created.')\n",
    "                return\n",
    "\n",
    "            print('gdf has at least 1 polygon')\n",
    "\n",
    "            # Remove polygons with centroids that are outside the tile boundary\n",
    "            prop_cent_in_tile = self.config.polygon_prop(\n",
    "                'centroid_within_tile')\n",
    "            gdf = gdf[gdf[prop_cent_in_tile]]\n",
    "\n",
    "            # Check if deduplication should be performed\n",
    "            dedup_here = self.config.deduplicate_at('3dtiles')\n",
    "            dedup_method = self.config.get_deduplication_method()\n",
    "\n",
    "            # Deduplicate if required\n",
    "            if dedup_here and (dedup_method is not None):\n",
    "                dedup_config = self.config.get_deduplication_config(gdf)\n",
    "                dedup = dedup_method(gdf, **dedup_config)\n",
    "                gdf = dedup['keep']\n",
    "\n",
    "                # The tile could theoretically be empty after deduplication\n",
    "                if len(gdf) == 0:\n",
    "                    logger.warning(\n",
    "                        f'Vector tile {path} is empty after deduplication.'\n",
    "                        ' 3D Tile will not be created.')\n",
    "                    return\n",
    "\n",
    "            # Create & save the b3dm file\n",
    "            ces_tile, ces_tileset = TreeGenerator.leaf_tile_from_gdf(\n",
    "                gdf,\n",
    "                dir=tile_dir,\n",
    "                filename=tile_filename,\n",
    "                z=self.config.get('z_coord'),\n",
    "                geometricError=self.config.get('geometricError'),\n",
    "                tilesetVersion=self.config.get('version'),\n",
    "                boundingVolume=tile_bv\n",
    "            )\n",
    "\n",
    "            return ces_tile, ces_tileset\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f'Error creating 3D Tile from {path}.')\n",
    "            logger.error(e)\n",
    "\n",
    "    def parent_3dtiles_from_children(self, tiles, bv_limit=None):\n",
    "        \"\"\"\n",
    "            Create parent Cesium 3D Tileset json files that point to of child\n",
    "            JSON files in the tile tree hierarchy. This method will take a list\n",
    "            of parent tiles and search the 3D tile directory for any children\n",
    "            tiles to create.\n",
    "            Parameters\n",
    "            ----------\n",
    "            tiles : list of morecantile.Tile\n",
    "                The list of parent tiles to create.\n",
    "        \"\"\"\n",
    "\n",
    "        tile_manager = self.tiles\n",
    "        config_manager = self.config\n",
    "\n",
    "        tileset_objs = []\n",
    "\n",
    "        # Make the next level of parent tiles\n",
    "        for parent_tile in tiles:\n",
    "            # Get the path to the parent tile\n",
    "            parent_path = tile_manager.path_from_tile(parent_tile, '3dtiles')\n",
    "            # Get just the base dir without the filename\n",
    "            parent_dir = os.path.dirname(parent_path)\n",
    "            # Get the filename of the parent tile, without the extension\n",
    "            parent_filename = os.path.basename(parent_path)\n",
    "            parent_filename = os.path.splitext(parent_filename)[0]\n",
    "            # Get the children paths for this parent tile\n",
    "            child_paths = tile_manager.get_child_paths(parent_tile, '3dtiles')\n",
    "            # Remove paths that do not exist\n",
    "            child_paths = tile_manager.remove_nonexistent_paths(child_paths)\n",
    "            # Get the parent bounding volume\n",
    "            parent_bv = self.bounding_region_for_tile(\n",
    "                parent_tile, limit_to=bv_limit)\n",
    "            # If the bounding region is outside t\n",
    "            # Get the version\n",
    "            version = config_manager.get('version')\n",
    "            # Get the geometric error\n",
    "            geometric_error = config_manager.get('geometricError')\n",
    "            # Create the parent tile\n",
    "            tileset_obj = TreeGenerator.parent_tile_from_children_json(\n",
    "                child_paths,\n",
    "                dir=parent_dir,\n",
    "                filename=parent_filename,\n",
    "                geometricError=geometric_error,\n",
    "                tilesetVersion=version,\n",
    "                boundingVolume=parent_bv\n",
    "            )\n",
    "            tileset_objs.append(tileset_obj)\n",
    "\n",
    "        return tileset_objs\n",
    "\n",
    "    def bounding_region_for_tile(self, tile, limit_to=None):\n",
    "        \"\"\"\n",
    "        For a morecantile.Tile object, return a BoundingVolumeRegion object\n",
    "        that represents the bounding region of the tile.\n",
    "        Parameters\n",
    "        ----------\n",
    "        tile : morecantile.Tile\n",
    "            The tile object.\n",
    "        limit_to : list of float\n",
    "            Optional list of west, south, east, north coordinates to limit\n",
    "            the bounding region to.\n",
    "        Returns\n",
    "        -------\n",
    "        bv : BoundingVolumeRegion\n",
    "            The bounding region object.\n",
    "        \"\"\"\n",
    "        tms = self.tiles.tms\n",
    "        bounds = tms.bounds(tile)\n",
    "        bounds = gpd.GeoSeries(\n",
    "            box(bounds.left, bounds.bottom, bounds.right, bounds.top),\n",
    "            crs=tms.crs)\n",
    "        if limit_to is not None:\n",
    "            bounds_limitor = gpd.GeoSeries(\n",
    "                box(limit_to[0], limit_to[1], limit_to[2], limit_to[3]),\n",
    "                crs=tms.crs)\n",
    "            bounds = bounds.intersection(bounds_limitor)\n",
    "        bounds = bounds.to_crs(BoundingVolumeRegion.CESIUM_EPSG)\n",
    "        bounds = bounds.total_bounds\n",
    "\n",
    "        region_bv = {\n",
    "            'west': bounds[0], 'south': bounds[1],\n",
    "            'east': bounds[2], 'north': bounds[3],\n",
    "        }\n",
    "        return region_bv\n",
    "\n",
    "    def make_top_level_tileset(self):\n",
    "        \"\"\"\n",
    "        Create a top-level tileset.json file that sets all the min_z level\n",
    "        tiles as its children. This is needed to display the tiles in Cesium\n",
    "        when the min_z level has more than one tile.\n",
    "        Returns\n",
    "        -------\n",
    "        tileset : Tileset\n",
    "            The Cesium3DTileset object\n",
    "        \"\"\"\n",
    "\n",
    "        tile_manager = self.tiles\n",
    "        config_manager = self.config\n",
    "        min_z = config_manager.get_min_z()\n",
    "\n",
    "        # Make a parent tileset.json - this will combine the top level tiles if\n",
    "        # there are 2, otherwise it will just refer to the top level tile.\n",
    "        top_level_tiles = tile_manager.get_filenames_from_dir(\n",
    "            '3dtiles', z=min_z)\n",
    "        top_level_dir = tile_manager.get_base_dir('3dtiles')['path']\n",
    "\n",
    "        return TreeGenerator.parent_tile_from_children_json(\n",
    "            children=top_level_tiles,\n",
    "            dir=top_level_dir\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logging(log_json_file):\n",
    "    \"\"\"\n",
    "    Setup logging configuration\n",
    "    \"\"\"\n",
    "    with open(log_json_file, 'r') as f:\n",
    "        logging_dict = json.load(f)\n",
    "    logging.config.dictConfig(logging_dict)\n",
    "    return logging_dict\n",
    "\n",
    "logging_dict = setup_logging(logging_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define batch sizes and batching function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_staging=1 # change this depending on data sample size!!!!!\n",
    "batch_size_rasterization=30\n",
    "batch_size_3dtiles=20\n",
    "batch_size_parent_3dtiles=500\n",
    "batch_size_geotiffs=200\n",
    "batch_size_web_tiles=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(items, batch_size):\n",
    "    \"\"\"\n",
    "    Create batches of a given size from a list of items.\n",
    "    \"\"\"\n",
    "    return [items[i:i + batch_size] for i in range(0, len(items), batch_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create batches of input files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk written when using original 3 gpkg files\n",
    "#input\n",
    "# this already is the paths to the input files, it is not a base dir\n",
    "# so we do not have to use stager.tiles.get_filenames_from_dir('input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batches = make_batch(input, batch_size_staging)\n",
    "input_batches # 3 batches, 1 file each\n",
    "\n",
    "# when batch size is 1, input_batches = input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the stager, raster tiler, and 3d tiler\n",
    "\n",
    "Even tho these objects need to be created within the `parsl` functions when we define those, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# staging configuration\n",
    "stager = pdgstaging.TileStager(workflow_config)\n",
    "tile_manager = stager.tiles\n",
    "config_manager = stager.config\n",
    "\n",
    "# zoom levels configuration\n",
    "min_z = config_manager.get_min_z()\n",
    "max_z = config_manager.get_max_z()\n",
    "parent_zs = range(max_z - 1, min_z - 1, -1)\n",
    "\n",
    "# 3D tiler configuration\n",
    "tiles3dmaker = StagedTo3DConverter(workflow_config)\n",
    "\n",
    "# raster tilerconfiguration \n",
    "rasterizer = pdgraster.RasterTiler(workflow_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up parsl app to stage in parallel\n",
    "\n",
    "Need to import all necessary packages for the staging step within parsl app for staging.\n",
    "\n",
    "I am actually not going to stage in parallel for this run thru, because there are so few input files, it messes with batching because there is only 1 gpkg per batch. I will batch for rasterization and web tiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decorators seem to be ignored as the first line of a cell, so print something first\n",
    "print(\"Stage in parallel\")\n",
    "\n",
    "@python_app\n",
    "def stage(paths, config, logging_dict = logging_dict): \n",
    "    \"\"\"\n",
    "    Stage files (step 1)\n",
    "    \"\"\"\n",
    "    import pdgstaging\n",
    "    if logging_dict:\n",
    "        import logging.config\n",
    "        logging.config.dictConfig(logging_dict)\n",
    "    stager = pdgstaging.TileStager(config)\n",
    "    for path in paths:\n",
    "        stager.stage(path)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage input files in parallel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_futures = []\n",
    "for batch in input_batches:\n",
    "    app_future = stage(batch, workflow_config, logging_dict)\n",
    "    #print(app_future)\n",
    "    app_futures.append(app_future)\n",
    "\n",
    "#Don't continue to step 2 until all files have been staged, only need the next line if running a script\n",
    "#[a.result() for a in app_futures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "htex_config_local.executors[0].shutdown()\n",
    "parsl.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch staged filepaths\n",
    "\n",
    "Now the staged file dir is complete. Moving onto preparing the staged files for rasterization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get paths to all the newly staged tiles\n",
    "staged_paths = stager.tiles.get_filenames_from_dir('staged')\n",
    "staged_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many staged files, is the batch size 30 reasonable? 19088... sure 30 sounds fine\n",
    "len(staged_paths) # matches the terminal count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch staged files\n",
    "staged_batches = make_batch(staged_paths, batch_size_rasterization)\n",
    "len(staged_batches) # 634 batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see what is within 1 batch\n",
    "staged_batches[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define parsl function to rasterize in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decorators seem to be ignored as the first line of a cell, so print something first\n",
    "print(\"Rasterize in parallel\")\n",
    "\n",
    "@python_app\n",
    "def rasterize(staged_paths, config, logging_dict = logging_dict):\n",
    "    \"\"\"\n",
    "    Rasterize a batch of vector files (step 2)\n",
    "    \"\"\"\n",
    "    import pdgraster\n",
    "    if logging_dict:\n",
    "        import logging.config\n",
    "        logging.config.dictConfig(logging_dict)\n",
    "    rasterizer = pdgraster.RasterTiler(config)\n",
    "    raster = rasterizer.rasterize_vectors(staged_paths, make_parents=True)\n",
    "    # print(tile)\n",
    "    # print(bounds)\n",
    "    # print(raster_opts)\n",
    "    return raster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rasterize all staged tiles (only highest z-level)\n",
    "\n",
    "First, reload parsl config because already cleared it after staging in parallel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bash command to activate virtual environment\n",
    "activate_env = 'source /home/jcohen/.bashrc; conda activate pdgviz'\n",
    "\n",
    "htex_config_local = Config(\n",
    "  executors = [\n",
    "      HighThroughputExecutor(\n",
    "        label = \"htex_Local\",\n",
    "        cores_per_worker = 2, \n",
    "        max_workers = 2, # why would this be so low? because just testing with small amount of data ?\n",
    "          # worker_logdir_root = '/' only necessary if the file system is remote, which is not the case for this lake change sample\n",
    "          # address not necessary because we are not using kubernetes\n",
    "        worker_debug = False, # don't need this because we have logging setup\n",
    "          # provider is local for this run thru, kubernetes would use KubernetesProvider()\n",
    "        provider = LocalProvider(\n",
    "          channel = LocalChannel(),\n",
    "          worker_init = activate_env,\n",
    "          init_blocks = 1, # default I think\n",
    "          max_blocks = 10 # changed from deafult of 1\n",
    "        ),\n",
    "      )\n",
    "    ],\n",
    "  )\n",
    "\n",
    "parsl.clear() # first clear the current configuration since we will likely run this script multiple times\n",
    "parsl.load(htex_config_local) # load the config we just outlined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_futures = []\n",
    "for batch in staged_batches:\n",
    "    app_future = rasterize(batch, workflow_config, logging_dict)\n",
    "    app_futures.append(app_future)\n",
    "\n",
    "# Don't continue to step 3 until all tiles have been rasterized\n",
    "[a.result() for a in app_futures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "htex_config_local.executors[0].shutdown()\n",
    "parsl.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure we have the same number of GeoTIFF files in z-level 11 as we do staged vector tiles (which are only z-level 11)\n",
    "#geotiff_paths = rasterizer.tiles.get_filenames_from_dir('/home/jcohen/viz-workflow/geotiff/WorldCRS84Quad/11')\n",
    "# just want geotiff paths from z-level 11 because all other z-levels are parent tiles, but just count number of tiles in terminal cause it's easier\n",
    "len_geotiff_paths = 18991\n",
    "len_geotiff_paths == len(staged_paths)\n",
    "# False with 1 file erroring, ambigious message, gonna ignore that file for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(staged_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Web Tiles from geoTIFF's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update ranges\n",
    "rasterizer.update_ranges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process web tiles in batches\n",
    "geotiff_paths = tile_manager.get_filenames_from_dir('geotiff')\n",
    "geotiff_batches = make_batch(geotiff_paths, batch_size_web_tiles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(geotiff_batches) # 134 batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(geotiff_batches[0]) # 200 in each batch, a huge increase from the batch size of 30 when rasterizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-initialize parsl\n",
    "\n",
    "Because I shut it down after rasterization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bash command to activate virtual environment\n",
    "activate_env = 'source /home/jcohen/.bashrc; conda activate pdgviz'\n",
    "\n",
    "htex_config_local = Config(\n",
    "  executors = [\n",
    "      HighThroughputExecutor(\n",
    "        label = \"htex_Local\",\n",
    "        cores_per_worker = 2, \n",
    "        max_workers = 2, # why would this be so low? because just testing with small amount of data ?\n",
    "          # worker_logdir_root = '/' only necessary if the file system is remote, which is not the case for this lake change sample\n",
    "          # address not necessary because we are not using kubernetes\n",
    "        worker_debug = False, # don't need this because we have logging setup\n",
    "          # provider is local for this run thru, kubernetes would use KubernetesProvider()\n",
    "        provider = LocalProvider(\n",
    "          channel = LocalChannel(),\n",
    "          worker_init = activate_env,\n",
    "          init_blocks = 1, # default I think\n",
    "          max_blocks = 10 # changed from deafult of 1\n",
    "        ),\n",
    "      )\n",
    "    ],\n",
    "  )\n",
    "\n",
    "parsl.clear() # first clear the current configuration since we will likely run this script multiple times\n",
    "parsl.load(htex_config_local) # load the config we just outlined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a batch of webtiles from geotiffs (step 4)\n",
    "@python_app\n",
    "def create_web_tiles(geotiff_paths, config, logging_dict = logging_dict):\n",
    "    \"\"\"\n",
    "    Create a batch of webtiles from geotiffs\n",
    "    \"\"\"\n",
    "    import pdgraster\n",
    "    if logging_dict:\n",
    "        import logging.config\n",
    "        logging.config.dictConfig(logging_dict)\n",
    "    rasterizer = pdgraster.RasterTiler(config)\n",
    "    return rasterizer.webtiles_from_geotiffs(\n",
    "        geotiff_paths, update_ranges=False) # already manually updates ranges in chunk above, don't need to do it twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_futures = []\n",
    "for batch in geotiff_batches:\n",
    "    app_future = create_web_tiles(batch, workflow_config, logging_dict)\n",
    "    app_futures.append(app_future)\n",
    "\n",
    "# Don't record end time until all web tiles have been created\n",
    "[a.result() for a in app_futures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that the same amount of web tiles and rasters\n",
    "geotiff_paths = rasterizer.tiles.get_filenames_from_dir('geotiff')\n",
    "len(geotiff_paths) #26656\n",
    "# 53312 total, for coverage and polygon_count\n",
    "53312/2 # perfect! 26656"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create 3D Web Tiles\n",
    "\n",
    "At [this](https://github.com/PermafrostDiscoveryGateway/viz-workflow/blob/0beb3b14239f2dd8cd4329026dc8d9a41aece7d7/pdg_workflow/pdg_workflow.py#L226) stage in the `parsl` workflow.\n",
    "\n",
    "We create only the highest z-level. Deduplication does not occur at this step, because we set the config for this workflow to occur at staging. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "staged_batches = make_batch(staged_paths, batch_size_3dtiles) # batch size = 20\n",
    "len(staged_batches) # 950 batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Include 3 things within the `parsl` app:\n",
    "\n",
    "1. Manually define class for StagedTo3DConverter\n",
    "2. Import TreeGenerator to create leaf tile in a Cesium 3D tileset tree with `leaf_tile_from_gdf()` and create a parent tile in a Cesium 3D tileset tree\n",
    "3. Import BoundingVolumeRegion\n",
    "\n",
    "\n",
    "[TreeGenerator script](https://github.com/PermafrostDiscoveryGateway/viz-3dtiles/blob/5597407f74cb4200d776dcc0185b7a67b73693fa/viz_3dtiles/TreeGenerator.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"create 3d webtiles in parallel\")\n",
    "\n",
    "@python_app\n",
    "def create_leaf_3dtiles(staged_paths, config, logging_dict = logging_dict):\n",
    "    \"\"\"\n",
    "    Create a batch of leaf 3d tiles from staged vector tiles\n",
    "    \"\"\"\n",
    "\n",
    "    print('Importing pdgstaging and viz_3dtiles')\n",
    "\n",
    "    import pdgstaging\n",
    "    from viz_3dtiles import TreeGenerator, BoundingVolumeRegion\n",
    "    \n",
    "    # from pdg_workflow import StagedTo3DConverter - manually define it instead:\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "\n",
    "    print('defining StagedTo3DConverter')\n",
    "\n",
    "    class StagedTo3DConverter():\n",
    "        \"\"\"\n",
    "        Processes staged vector data into Cesium 3D tiles according to the\n",
    "        settings in a config file or dict. This class acts as the orchestrator\n",
    "        of the other viz-3dtiles classes, and coordinates the sending and\n",
    "        receiving of information between them.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(\n",
    "            self,\n",
    "            config\n",
    "        ):\n",
    "            \"\"\"\n",
    "                Initialize the StagedTo3DConverter class.\n",
    "                Parameters\n",
    "                ----------\n",
    "                config : dict or str\n",
    "                    A dictionary of configuration settings or a path to a config\n",
    "                    JSON file. (See help(pdgstaging.ConfigManager))\n",
    "            \"\"\"\n",
    "\n",
    "            self.config = pdgstaging.ConfigManager(config)\n",
    "            self.tiles = pdgstaging.TilePathManager(\n",
    "                **self.config.get_path_manager_config())\n",
    "\n",
    "        def all_staged_to_3dtiles(\n",
    "            self\n",
    "        ):\n",
    "            \"\"\"\n",
    "                Process all staged vector tiles into 3D tiles.\n",
    "            \"\"\"\n",
    "\n",
    "            # Get the list of staged vector tiles\n",
    "            paths = self.tiles.get_filenames_from_dir('staged')\n",
    "            # Process each tile\n",
    "            for path in paths:\n",
    "                self.staged_to_3dtile(path)\n",
    "\n",
    "        def staged_to_3dtile(self, path):\n",
    "            \"\"\"\n",
    "                Convert a staged vector tile into a B3DM tile file and a matching\n",
    "                JSON tileset file.\n",
    "                Parameters\n",
    "                ----------\n",
    "                path : str\n",
    "                    The path to the staged vector tile.\n",
    "                Returns\n",
    "                -------\n",
    "                tile, tileset : Cesium3DTile, Tileset\n",
    "                    The Cesium3DTiles and Cesium3DTileset objects\n",
    "            \"\"\"\n",
    "\n",
    "            print('about to try to get tile from path')\n",
    "\n",
    "            try:\n",
    "\n",
    "                # Get information about the tile from the path\n",
    "                tile = self.tiles.tile_from_path(path)\n",
    "\n",
    "                print(f'Tile being processed is {tile}')\n",
    "\n",
    "                out_path = self.tiles.path_from_tile(tile, '3dtiles')\n",
    "\n",
    "                print(f'Path to write 3D tile will be {out_path}')\n",
    "\n",
    "                tile_bv = self.bounding_region_for_tile(tile)\n",
    "\n",
    "                print(f'Bounding region for tile is {tile_bv}')\n",
    "\n",
    "                # Get the filename of the tile WITHOUT the extension\n",
    "                tile_filename = os.path.splitext(os.path.basename(out_path))[0]\n",
    "\n",
    "                print(f'filename for tile is {tile_filename}')\n",
    "\n",
    "                # Get the base of the path, without the filename\n",
    "                tile_dir = os.path.dirname(out_path) + os.path.sep\n",
    "\n",
    "                print(f'directory for tile is {tile_dir}')\n",
    "\n",
    "                # Log the event\n",
    "                logger.info(\n",
    "                    f'Creating 3dtile from {path} for tile {tile} to {out_path}.')\n",
    "\n",
    "                # Read in the staged vector tile\n",
    "                gdf = gpd.read_file(path)\n",
    "\n",
    "                # Check if the gdf is empty\n",
    "                if len(gdf) == 0:\n",
    "                    logger.warning(\n",
    "                        f'Vector tile {path} is empty. 3D tile will not be'\n",
    "                        ' created.')\n",
    "                    return\n",
    "\n",
    "                # Remove polygons with centroids that are outside the tile boundary\n",
    "                prop_cent_in_tile = self.config.polygon_prop(\n",
    "                    'centroid_within_tile')\n",
    "                gdf = gdf[gdf[prop_cent_in_tile]]\n",
    "\n",
    "                # Check if deduplication should be performed\n",
    "                dedup_here = self.config.deduplicate_at('3dtiles')\n",
    "                dedup_method = self.config.get_deduplication_method()\n",
    "\n",
    "                # Deduplicate if required\n",
    "                if dedup_here and (dedup_method is not None):\n",
    "                    dedup_config = self.config.get_deduplication_config(gdf)\n",
    "                    dedup = dedup_method(gdf, **dedup_config)\n",
    "                    gdf = dedup['keep']\n",
    "\n",
    "                    # The tile could theoretically be empty after deduplication\n",
    "                    if len(gdf) == 0:\n",
    "                        logger.warning(\n",
    "                            f'Vector tile {path} is empty after deduplication.'\n",
    "                            ' 3D Tile will not be created.')\n",
    "                        return\n",
    "\n",
    "                # Create & save the b3dm file\n",
    "                ces_tile, ces_tileset = TreeGenerator.leaf_tile_from_gdf(\n",
    "                    gdf,\n",
    "                    dir=tile_dir,\n",
    "                    filename=tile_filename,\n",
    "                    z=self.config.get('z_coord'),\n",
    "                    geometricError=self.config.get('geometricError'),\n",
    "                    tilesetVersion=self.config.get('version'),\n",
    "                    boundingVolume=tile_bv\n",
    "                )\n",
    "\n",
    "                return ces_tile, ces_tileset\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f'Error creating 3D Tile from {path}.')\n",
    "                logger.error(e)\n",
    "\n",
    "        def parent_3dtiles_from_children(self, tiles, bv_limit=None):\n",
    "            \"\"\"\n",
    "                Create parent Cesium 3D Tileset json files that point to of child\n",
    "                JSON files in the tile tree hierarchy. This method will take a list\n",
    "                of parent tiles and search the 3D tile directory for any children\n",
    "                tiles to create.\n",
    "                Parameters\n",
    "                ----------\n",
    "                tiles : list of morecantile.Tile\n",
    "                    The list of parent tiles to create.\n",
    "            \"\"\"\n",
    "\n",
    "            tile_manager = self.tiles\n",
    "            config_manager = self.config\n",
    "\n",
    "            tileset_objs = []\n",
    "\n",
    "            # Make the next level of parent tiles\n",
    "            for parent_tile in tiles:\n",
    "                # Get the path to the parent tile\n",
    "                parent_path = tile_manager.path_from_tile(parent_tile, '3dtiles')\n",
    "                # Get just the base dir without the filename\n",
    "                parent_dir = os.path.dirname(parent_path)\n",
    "                # Get the filename of the parent tile, without the extension\n",
    "                parent_filename = os.path.basename(parent_path)\n",
    "                parent_filename = os.path.splitext(parent_filename)[0]\n",
    "                # Get the children paths for this parent tile\n",
    "                child_paths = tile_manager.get_child_paths(parent_tile, '3dtiles')\n",
    "                # Remove paths that do not exist\n",
    "                child_paths = tile_manager.remove_nonexistent_paths(child_paths)\n",
    "                # Get the parent bounding volume\n",
    "                parent_bv = self.bounding_region_for_tile(\n",
    "                    parent_tile, limit_to=bv_limit)\n",
    "                # If the bounding region is outside t\n",
    "                # Get the version\n",
    "                version = config_manager.get('version')\n",
    "                # Get the geometric error\n",
    "                geometric_error = config_manager.get('geometricError')\n",
    "                # Create the parent tile\n",
    "                tileset_obj = TreeGenerator.parent_tile_from_children_json(\n",
    "                    child_paths,\n",
    "                    dir=parent_dir,\n",
    "                    filename=parent_filename,\n",
    "                    geometricError=geometric_error,\n",
    "                    tilesetVersion=version,\n",
    "                    boundingVolume=parent_bv\n",
    "                )\n",
    "                tileset_objs.append(tileset_obj)\n",
    "\n",
    "            return tileset_objs\n",
    "\n",
    "        def bounding_region_for_tile(self, tile, limit_to=None):\n",
    "            \"\"\"\n",
    "            For a morecantile.Tile object, return a BoundingVolumeRegion object\n",
    "            that represents the bounding region of the tile.\n",
    "            Parameters\n",
    "            ----------\n",
    "            tile : morecantile.Tile\n",
    "                The tile object.\n",
    "            limit_to : list of float\n",
    "                Optional list of west, south, east, north coordinates to limit\n",
    "                the bounding region to.\n",
    "            Returns\n",
    "            -------\n",
    "            bv : BoundingVolumeRegion\n",
    "                The bounding region object.\n",
    "            \"\"\"\n",
    "            tms = self.tiles.tms\n",
    "            bounds = tms.bounds(tile)\n",
    "            bounds = gpd.GeoSeries(\n",
    "                box(bounds.left, bounds.bottom, bounds.right, bounds.top),\n",
    "                crs=tms.crs)\n",
    "            if limit_to is not None:\n",
    "                bounds_limitor = gpd.GeoSeries(\n",
    "                    box(limit_to[0], limit_to[1], limit_to[2], limit_to[3]),\n",
    "                    crs=tms.crs)\n",
    "                bounds = bounds.intersection(bounds_limitor)\n",
    "            bounds = bounds.to_crs(BoundingVolumeRegion.CESIUM_EPSG)\n",
    "            bounds = bounds.total_bounds\n",
    "\n",
    "            region_bv = {\n",
    "                'west': bounds[0], 'south': bounds[1],\n",
    "                'east': bounds[2], 'north': bounds[3],\n",
    "            }\n",
    "            return region_bv\n",
    "\n",
    "        def make_top_level_tileset(self):\n",
    "            \"\"\"\n",
    "            Create a top-level tileset.json file that sets all the min_z level\n",
    "            tiles as its children. This is needed to display the tiles in Cesium\n",
    "            when the min_z level has more than one tile.\n",
    "            Returns\n",
    "            -------\n",
    "            tileset : Tileset\n",
    "                The Cesium3DTileset object\n",
    "            \"\"\"\n",
    "\n",
    "            tile_manager = self.tiles\n",
    "            config_manager = self.config\n",
    "            min_z = config_manager.get_min_z()\n",
    "\n",
    "            # Make a parent tileset.json - this will combine the top level tiles if\n",
    "            # there are 2, otherwise it will just refer to the top level tile.\n",
    "            top_level_tiles = tile_manager.get_filenames_from_dir(\n",
    "                '3dtiles', z=min_z)\n",
    "            top_level_dir = tile_manager.get_base_dir('3dtiles')['path']\n",
    "\n",
    "            return TreeGenerator.parent_tile_from_children_json(\n",
    "                children=top_level_tiles,\n",
    "                dir=top_level_dir\n",
    "            )\n",
    "\n",
    "    print('Setting up logging')\n",
    "\n",
    "    if logging_dict:\n",
    "        import logging.config\n",
    "        logging.config.dictConfig(logging_dict)\n",
    "\n",
    "    print('defining config from StagedTo3DConverter')\n",
    "\n",
    "    converter3d = StagedTo3DConverter(config)\n",
    "    tilesets = []\n",
    "    for path in staged_paths:\n",
    "        ces_tile, ces_tileset = converter3d.staged_to_3dtile(path)\n",
    "        tilesets.append(ces_tileset)\n",
    "    return tilesets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_futures = []\n",
    "for batch in staged_batches:\n",
    "    app_future = create_leaf_3dtiles(staged_paths = batch, config = workflow_config, logging_dict = logging_dict)\n",
    "    app_futures.append(app_future)\n",
    "\n",
    "# Don't continue to step 6 until all max-zoom level 3d tilesets have been created\n",
    "[a.result() for a in app_futures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "htex_config_local.executors[0].shutdown()\n",
    "parsl.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset the staged_paths in order to test if runnning function on single paths rather than batches works\n",
    "staged_paths_subset = staged_paths[0:100] # 100 paths to individual staged files\n",
    "len(staged_paths_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test to see if the function is erroring because we feed in batch instead of 1 path, might need to insert another loop to loop thru paths within each batch\n",
    "# app_futures = []\n",
    "# for path in staged_paths_subset:\n",
    "#     app_future = create_leaf_3dtiles(path, workflow_config, logging_dict)\n",
    "#     app_futures.append(app_future)\n",
    "\n",
    "# # Don't continue to step 6 until all max-zoom level 3d tilesets have been created\n",
    "# [a.result() for a in app_futures]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Web tiles NOT in parallel - to troubleshoot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_leaf_3dtiles(staged_paths, config, logging_dict=None):\n",
    "    \"\"\"\n",
    "    Create a batch of leaf 3d tiles from staged vector tiles\n",
    "    \"\"\"\n",
    "    #from pdg_workflow import StagedTo3DConverter\n",
    "\n",
    "    if logging_dict:\n",
    "        import logging.config\n",
    "        logging.config.dictConfig(logging_dict)\n",
    "    converter3d = StagedTo3DConverter(config)\n",
    "    tilesets = []\n",
    "    for path in staged_paths:\n",
    "        try:\n",
    "            ces_tile, ces_tileset = converter3d.staged_to_3dtile(path)\n",
    "            tilesets.append(ces_tileset)\n",
    "        except Exception as e:\n",
    "            logging.error(f'Error creating 3d tile from {path}')\n",
    "            logging.error(e)\n",
    "    return tilesets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in staged_batches:\n",
    "    create_leaf_3dtiles(batch, workflow_config, logging_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to troubleshoot error resutling from 3d tiling - might be due to package version dependencies changing when installed new parsl package for monitoring parsl activity in parallle - make new virtual env and start process over from staging, with fresh versions of csv's: staging_summary.csv, rasterization_events.csv, and rasters_summary.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('pdgViz': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "10a2e56ef656d98ef0421af51926b942cbb8e38069ff79c3b0ecde222001ebd1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
