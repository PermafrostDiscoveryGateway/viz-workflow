{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsl Workflow with Ingmar's lake Change Sample Data \n",
    "\n",
    "- gpkg files\n",
    "- new TMS to work with GeoPandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "# file paths\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import box\n",
    "\n",
    "# PDG packages\n",
    "import pdgstaging\n",
    "import pdgraster\n",
    "import py3dtiles\n",
    "import viz_3dtiles\n",
    "from viz_3dtiles import TreeGenerator, BoundingVolumeRegion\n",
    "#import pdgpy3dtiles\n",
    "#from StagedTo3DConverter import StagedTo3DConverter\n",
    "\n",
    "# logging and configuration\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import logging.config\n",
    "import argparse\n",
    "import json\n",
    "\n",
    "# Parsl\n",
    "import parsl\n",
    "from parsl import python_app\n",
    "from parsl.config import Config\n",
    "from parsl.channels import LocalChannel\n",
    "from parsl.executors import HighThroughputExecutor\n",
    "from parsl.providers import LocalProvider\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "Note: Robyn figured out that using TMS WGS1984Quad instead of WorldCRS84Quad fixes the geopandas issue, but have not switched the config yet.\n",
    "\n",
    "Config sourced in for this workflow: `ingmar_config.json`\n",
    "\n",
    "```\n",
    "{\n",
    "    \"simplify_tolerance\": 0.0001,\n",
    "    \"tms_id\": \"WGS1984Quad\",\n",
    "    \"z_range\": [0, 11],\n",
    "    \"statistics\": [\n",
    "        {\n",
    "            \"name\": \"polygon_count\",\n",
    "            \"weight_by\": \"count\",\n",
    "            \"property\": \"centroids_per_pixel\",\n",
    "            \"aggregation_method\": \"sum\",\n",
    "            \"resampling_method\": \"sum\",\n",
    "            \"val_range\": [0, null],\n",
    "            \"nodata_val\": 0,\n",
    "            \"nodata_color\": \"#ffffff00\",\n",
    "            \"palette\": [\"#d9c43f\", \"#d93fce\"]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"coverage\",\n",
    "            \"weight_by\": \"area\",\n",
    "            \"property\": \"area_per_pixel_area\",\n",
    "            \"aggregation_method\": \"sum\",\n",
    "            \"resampling_method\": \"average\",\n",
    "            \"val_range\": [0, 1],\n",
    "            \"nodata_val\": 0,\n",
    "            \"nodata_color\": \"#ffffff00\",\n",
    "            \"palette\": [\"#d9c43f\", \"#d93fce\"]\n",
    "        }\n",
    "    ],\n",
    "    \"deduplicate_at\": [\"staging\"],\n",
    "    \"deduplicate_method\": \"neighbor\",\n",
    "    \"deduplicate_keep_rules\": [[\"staging_filename\", \"larger\"]],\n",
    "    \"deduplicate_overlap_tolerance\": 0.1,\n",
    "    \"deduplicate_overlap_both\": false,\n",
    "    \"deduplicate_centroid_tolerance\": null\n",
    "  }\n",
    "  ```\n",
    "\n",
    "### Python Environment\n",
    "\n",
    "Using jcohen's virtual env `testing` - only has installed parsl and the PDG packages as well as their dependencies, but with geopandas version 0.11.1 to avoid the CRS error alert in the newest version of geopandas.\n",
    "\n",
    "Packages & versions used for this workflow:\n",
    "\n",
    "```\n",
    "Package              Version\n",
    "-------------------- -----------\n",
    "addict               2.4.0\n",
    "affine               2.3.1\n",
    "asttokens            2.1.0\n",
    "attrs                22.1.0\n",
    "backcall             0.2.0\n",
    "bcrypt               4.0.1\n",
    "certifi              2022.9.24\n",
    "cffi                 1.15.1\n",
    "charset-normalizer   2.1.1\n",
    "click                8.1.3\n",
    "click-plugins        1.1.1\n",
    "cligj                0.7.2\n",
    "coloraide            0.18.1\n",
    "colormaps            0.3\n",
    "comm                 0.1.0\n",
    "ConfigArgParse       1.5.3\n",
    "contourpy            1.0.6\n",
    "cryptography         38.0.3\n",
    "cycler               0.11.0\n",
    "Cython               0.29.32\n",
    "dash                 2.7.0\n",
    "dash-core-components 2.0.0\n",
    "dash-html-components 2.0.0\n",
    "dash-table           5.0.0\n",
    "debugpy              1.6.3\n",
    "decorator            5.1.1\n",
    "dill                 0.3.6\n",
    "entrypoints          0.4\n",
    "executing            1.2.0\n",
    "fastjsonschema       2.16.2\n",
    "filelock             3.8.0\n",
    "Fiona                1.8.22\n",
    "Flask                2.2.2\n",
    "fonttools            4.38.0\n",
    "geopandas            0.11.1\n",
    "globus-sdk           3.14.0\n",
    "idna                 3.4\n",
    "importlib-metadata   5.0.0\n",
    "ipykernel            6.18.0\n",
    "ipython              8.6.0\n",
    "ipywidgets           8.0.2\n",
    "itsdangerous         2.1.2\n",
    "jedi                 0.18.1\n",
    "Jinja2               3.1.2\n",
    "joblib               1.2.0\n",
    "jsonschema           4.17.0\n",
    "jupyter_client       7.4.7\n",
    "jupyter_core         5.0.0\n",
    "jupyterlab-widgets   3.0.3\n",
    "kiwisolver           1.4.4\n",
    "laspy                2.3.0\n",
    "llvmlite             0.39.1\n",
    "lz4                  4.0.2\n",
    "MarkupSafe           2.1.1\n",
    "matplotlib           3.6.2\n",
    "matplotlib-inline    0.1.6\n",
    "morecantile          3.2.1\n",
    "munch                2.5.0\n",
    "nbformat             5.5.0\n",
    "nest-asyncio         1.5.6\n",
    "numba                0.56.4\n",
    "numpy                1.22.4\n",
    "open3d               0.16.0\n",
    "packaging            21.3\n",
    "pandas               1.5.1\n",
    "paramiko             2.12.0\n",
    "parsl                2022.11.14\n",
    "parso                0.8.3\n",
    "pdgpy3dtiles         0.0.1\n",
    "pdgraster            0.1.0\n",
    "pdgstaging           0.1.0\n",
    "pexpect              4.8.0\n",
    "pickleshare          0.7.5\n",
    "Pillow               9.3.0\n",
    "pip                  22.2.2\n",
    "platformdirs         2.5.4\n",
    "plotly               5.11.0\n",
    "prompt-toolkit       3.0.33\n",
    "psutil               5.9.4\n",
    "psycopg2-binary      2.9.5\n",
    "ptyprocess           0.7.0\n",
    "pure-eval            0.2.2\n",
    "pycparser            2.21\n",
    "pydantic             1.10.2\n",
    "pygeos               0.13\n",
    "Pygments             2.13.0\n",
    "PyJWT                2.6.0\n",
    "PyNaCl               1.5.0\n",
    "pyparsing            3.0.9\n",
    "pyproj               3.4.0\n",
    "pyquaternion         0.9.9\n",
    "pyrsistent           0.19.2\n",
    "python-dateutil      2.8.2\n",
    "pytz                 2022.6\n",
    "PyYAML               6.0\n",
    "pyzmq                24.0.1\n",
    "rasterio             1.3.4\n",
    "requests             2.28.1\n",
    "Rtree                0.9.7\n",
    "scikit-learn         1.1.3\n",
    "scipy                1.9.3\n",
    "setproctitle         1.3.2\n",
    "setuptools           65.5.0\n",
    "Shapely              1.8.5.post1\n",
    "six                  1.16.0\n",
    "snuggs               1.4.7\n",
    "stack-data           0.6.1\n",
    "tblib                1.7.0\n",
    "tenacity             8.1.0\n",
    "threadpoolctl        3.1.0\n",
    "tornado              6.2\n",
    "tqdm                 4.64.1\n",
    "traitlets            5.5.0\n",
    "triangle             20220202\n",
    "typeguard            2.13.3\n",
    "typing_extensions    4.4.0\n",
    "urllib3              1.26.12\n",
    "viz-3dtiles          0.0.1\n",
    "wcwidth              0.2.5\n",
    "Werkzeug             2.2.2\n",
    "wheel                0.37.1\n",
    "widgetsnbextension   4.0.3\n",
    "zipp                 3.10.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lake change sample files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/pdg/data/nitze_lake_change/data_sample_2022-09-09/32609/05_Lake_Dataset_Raster_02_final/lake_change.gpkg',\n",
       " '/home/pdg/data/nitze_lake_change/data_sample_2022-09-09/32608/05_Lake_Dataset_Raster_02_final/lake_change.gpkg',\n",
       " '/home/pdg/data/nitze_lake_change/data_sample_2022-09-09/32607/05_Lake_Dataset_Raster_02_final/lake_change.gpkg']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_dir = Path('/home/pdg/data/nitze_lake_change/data_sample_2022-09-09')\n",
    "subdirs = ['32607', '32608', '32609']\n",
    "filename = 'lake_change.gpkg'\n",
    "#to define each .gpkg file within each UTM subdir as a string representation with forward slashes, use as_posix() for each iteration\n",
    "#of base_dir + filename. The ** represents that any subdir string can be present between the base_dir and the filename, meaning I do not\n",
    "#think that we needed to create the object subdirs above\n",
    "data_paths = [p.as_posix() for p in base_dir.glob('**/' + filename)]\n",
    "data_paths\n",
    "\n",
    "# smallest data sample for troubleshooting, 2 gpkg files with spatial overlap:\n",
    "# file1 = '/home/thiessenbock/PDG-test/minimal-example/input/file1.gpkg'\n",
    "# file2 = '/home/thiessenbock/PDG-test/minimal-example/input/file2.gpkg'\n",
    "# data_paths = [file1, file2]\n",
    "# data_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_config = '/home/jcohen/viz-workflow/workflow_troubleshooting/ingmar_config.json'\n",
    "\n",
    "# logging setup\n",
    "logging_config = '/home/jcohen/viz-workflow/workflow_troubleshooting/logging.json'\n",
    "\n",
    "def setup_logging(log_json_file):\n",
    "    \"\"\"\n",
    "    Setup logging configuration\n",
    "    \"\"\"\n",
    "    with open(log_json_file, 'r') as f:\n",
    "        logging_dict = json.load(f)\n",
    "    logging.config.dictConfig(logging_dict)\n",
    "    return logging_dict\n",
    "\n",
    "logging_dict = setup_logging(logging_config)\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StagedTo3DConverter():\n",
    "    \"\"\"\n",
    "        Processes staged vector data into Cesium 3D tiles according to the\n",
    "        settings in a config file or dict. This class acts as the orchestrator\n",
    "        of the other viz-3dtiles classes, and coordinates the sending and\n",
    "        receiving of information between them.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config\n",
    "    ):\n",
    "        \"\"\"\n",
    "            Automatically initialize the StagedTo3DConverter class by appying the configuration when an object of that class is created.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            self : need to explicitly state this parameter to pass any newly created object of class StagedTo3DConverter to the other paraneter (config)\n",
    "                this is a python syntax requirement in order for the object to persist of this class\n",
    "\n",
    "            config : dict or str\n",
    "                A dictionary of configuration settings or a path to a config\n",
    "                JSON file. (See help(pdgstaging.ConfigManager))\n",
    "\n",
    "            Notes\n",
    "            ----------\n",
    "            - this function does not do the staging or tiling steps\n",
    "        \"\"\"\n",
    "\n",
    "        self.config = pdgstaging.ConfigManager(config)\n",
    "        self.tiles = pdgstaging.TilePathManager(\n",
    "            **self.config.get_path_manager_config())\n",
    "\n",
    "    def all_staged_to_3dtiles(\n",
    "        self\n",
    "    ):\n",
    "        \"\"\"\n",
    "            Process all staged vector tiles into 3D tiles. This is simply a loop that iterates the function staged_to_rdtile() over all files in the staged directory.\n",
    "        \"\"\"\n",
    "\n",
    "        # Get the list of staged vector tiles\n",
    "        paths = self.tiles.get_filenames_from_dir('staged')\n",
    "        # Process each tile\n",
    "        for path in paths:\n",
    "            self.staged_to_3dtile(path)\n",
    "\n",
    "    def staged_to_3dtile(self, path):\n",
    "        \"\"\"\n",
    "            Convert a staged vector tile into a B3DM tile file and a matching\n",
    "            JSON tileset file.\n",
    "            - the B3DM tile is applied to the PDG portal for visualization purposes\n",
    "            - the JSON serves as the metadata for that tile\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            path : str\n",
    "                The path to the staged vector tile.\n",
    "\n",
    "            Returns\n",
    "            -------\n",
    "            tile, tileset : Cesium3DTile, Tileset\n",
    "                The Cesium3DTiles and Cesium3DTileset objects\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            \n",
    "            # Get information about the tile from the path\n",
    "            tile = self.tiles.tile_from_path(path)\n",
    "            out_path = self.tiles.path_from_tile(tile, '3dtiles')\n",
    "\n",
    "            tile_bv = self.bounding_region_for_tile(tile) # bv = bounding volumne\n",
    "\n",
    "            # Get the filename of the tile WITHOUT the extension\n",
    "            tile_filename = os.path.splitext(os.path.basename(out_path))[0]\n",
    "            # Get the base of the path, without the filename\n",
    "            tile_dir = os.path.dirname(out_path) + os.path.sep\n",
    "\n",
    "            # Log the event\n",
    "            logger.info(\n",
    "                f'Creating 3dtile from {path} for tile {tile} to {out_path}.')\n",
    "\n",
    "            # Read in the staged vector tile\n",
    "            gdf = gpd.read_file(path)\n",
    "\n",
    "            # Summary of following steps:\n",
    "            # Now that we have the path to the staged vector tile esptablished and logged, \n",
    "            # the following checks are executed on each staged vector tile:\n",
    "            # 1. check if the tile has any data to start with\n",
    "            # 2. check if the centroid of the polygons within the tile are within the tile boundaries, remove if not\n",
    "            # 3. check if polygons within the tile overlap, deduplicate them if they do\n",
    "            # 4. check if the tile has any data left if deduplication was executed\n",
    "            # 5. if there were errors in the above steps, log that for debugging\n",
    "\n",
    "            \n",
    "            # Check if the gdf is empty\n",
    "            if len(gdf) == 0:\n",
    "                logger.warning(\n",
    "                    f'Vector tile {path} is empty. 3D tile will not be'\n",
    "                    ' created.')\n",
    "                return\n",
    "\n",
    "            # Remove polygons with centroids that are outside the tile boundary\n",
    "            prop_cent_in_tile = self.config.polygon_prop(\n",
    "                'centroid_within_tile')\n",
    "            gdf = gdf[gdf[prop_cent_in_tile]]\n",
    "\n",
    "            # Check if deduplication should be performed\n",
    "            dedup_here = self.config.deduplicate_at('3dtiles')\n",
    "            dedup_method = self.config.get_deduplication_method()\n",
    "\n",
    "            # Deduplicate if required\n",
    "            if dedup_here and (dedup_method is not None):\n",
    "                dedup_config = self.config.get_deduplication_config(gdf)\n",
    "                dedup = dedup_method(gdf, **dedup_config)\n",
    "                gdf = dedup['keep']\n",
    "\n",
    "                # The tile could theoretically be empty after deduplication\n",
    "                if len(gdf) == 0:\n",
    "                    logger.warning(\n",
    "                        f'Vector tile {path} is empty after deduplication.'\n",
    "                        ' 3D Tile will not be created.')\n",
    "                    return\n",
    "\n",
    "            # Create & save the b3dm file\n",
    "            ces_tile, ces_tileset = TreeGenerator.leaf_tile_from_gdf(\n",
    "                gdf,\n",
    "                dir=tile_dir,\n",
    "                filename=tile_filename,\n",
    "                z=self.config.get('z_coord'),\n",
    "                geometricError=self.config.get('geometricError'),\n",
    "                tilesetVersion=self.config.get('version'),\n",
    "                boundingVolume=tile_bv\n",
    "            )\n",
    "\n",
    "            return ces_tile, ces_tileset\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f'Error creating 3D Tile from {path}.')\n",
    "            logger.error(e)\n",
    "\n",
    "    def parent_3dtiles_from_children(self, tiles, bv_limit=None):\n",
    "        \"\"\"\n",
    "            Create parent Cesium 3D Tileset json files that point to\n",
    "            of child JSON files in the tile tree hierarchy.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            tiles : list of morecantile.Tile\n",
    "                The list of tiles to create parent tiles for.\n",
    "        \"\"\"\n",
    "\n",
    "        tile_manager = self.tiles\n",
    "        config_manager = self.config\n",
    "\n",
    "        tileset_objs = []\n",
    "\n",
    "        # Make the next level of parent tiles\n",
    "        for parent_tile in tiles:\n",
    "            # Get the path to the parent tile\n",
    "            parent_path = tile_manager.path_from_tile(parent_tile, '3dtiles')\n",
    "            # Get just the base dir without the filename\n",
    "            parent_dir = os.path.dirname(parent_path)\n",
    "            # Get the filename of the parent tile, without the extension\n",
    "            parent_filename = os.path.basename(parent_path)\n",
    "            parent_filename = os.path.splitext(parent_filename)[0]\n",
    "            # Get the children paths for this parent tile\n",
    "            child_paths = tile_manager.get_child_paths(parent_tile, '3dtiles')\n",
    "            # Remove paths that do not exist\n",
    "            child_paths = tile_manager.remove_nonexistent_paths(child_paths)\n",
    "            # Get the parent bounding volume\n",
    "            parent_bv = self.bounding_region_for_tile(\n",
    "                parent_tile, limit_to=bv_limit)\n",
    "            # If the bounding region is outside t\n",
    "            # Get the version\n",
    "            version = config_manager.get('version')\n",
    "            # Get the geometric error\n",
    "            geometric_error = config_manager.get('geometricError')\n",
    "            # Create the parent tile\n",
    "            tileset_obj = TreeGenerator.parent_tile_from_children_json(\n",
    "                child_paths,\n",
    "                dir=parent_dir,\n",
    "                filename=parent_filename,\n",
    "                geometricError=geometric_error,\n",
    "                tilesetVersion=version,\n",
    "                boundingVolume=parent_bv\n",
    "            )\n",
    "            tileset_objs.append(tileset_obj)\n",
    "\n",
    "        return tileset_objs\n",
    "\n",
    "    def bounding_region_for_tile(self, tile, limit_to=None):\n",
    "        \"\"\"\n",
    "        For a morecantile.Tile object, return a BoundingVolumeRegion object\n",
    "        that represents the bounding region of the tile.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        tile : morecantile.Tile\n",
    "            The tile object.\n",
    "        limit_to : list of float\n",
    "            Optional list of west, south, east, north coordinates to limit\n",
    "            the bounding region to.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        bv : BoundingVolumeRegion\n",
    "            The bounding region object.\n",
    "        \"\"\"\n",
    "        tms = self.tiles.tms\n",
    "        bounds = tms.bounds(tile)\n",
    "        bounds = gpd.GeoSeries(\n",
    "            box(bounds.left, bounds.bottom, bounds.right, bounds.top),\n",
    "            crs=tms.crs)\n",
    "        if limit_to is not None:\n",
    "            bounds_limitor = gpd.GeoSeries(\n",
    "                box(limit_to[0], limit_to[1], limit_to[2], limit_to[3]),\n",
    "                crs=tms.crs)\n",
    "            bounds = bounds.intersection(bounds_limitor)\n",
    "        bounds = bounds.to_crs(BoundingVolumeRegion.CESIUM_EPSG)\n",
    "        bounds = bounds.total_bounds\n",
    "\n",
    "        region_bv = {\n",
    "            'west': bounds[0], 'south': bounds[1],\n",
    "            'east': bounds[2], 'north': bounds[3],\n",
    "        }\n",
    "        return region_bv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration files and creating stager, rasterizer, and tilers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# staging configuration\n",
    "stager = pdgstaging.TileStager(workflow_config)\n",
    "tile_manager = stager.tiles\n",
    "config_manager = stager.config\n",
    "\n",
    "# zoom levels configuration\n",
    "min_z = config_manager.get_min_z()\n",
    "max_z = config_manager.get_max_z()\n",
    "parent_zs = range(max_z - 1, min_z - 1, -1)\n",
    "\n",
    "# 3D tiler configuration\n",
    "tiles3dmaker = StagedTo3DConverter(workflow_config)\n",
    "\n",
    "# raster tilerconfiguration \n",
    "rasterizer = pdgraster.RasterTiler(workflow_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsl setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<parsl.dataflow.dflow.DataFlowKernel at 0x7fd8eeb88dc0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bash command to activate virtual environment\n",
    "activate_env = 'source /home/jcohen/.bashrc; conda activate testing'\n",
    "\n",
    "htex_config_local = Config(\n",
    "  executors = [\n",
    "      HighThroughputExecutor(\n",
    "        label = \"htex_Local\",\n",
    "        cores_per_worker = 2, \n",
    "        max_workers = 2, # why would this be so low? because just testing with small amount of data ?\n",
    "          # worker_logdir_root = '/' only necessary if the file system is remote, which is not the case for this lake change sample\n",
    "          # address not necessary because we are not using kubernetes\n",
    "        worker_debug = False, # don't need this because we have logging setup\n",
    "          # provider is local for this run thru, kubernetes would use KubernetesProvider()\n",
    "        provider = LocalProvider(\n",
    "          channel = LocalChannel(),\n",
    "          worker_init = activate_env,\n",
    "          init_blocks = 1, # default I think\n",
    "          max_blocks = 10 # changed from deafult of 1\n",
    "        ),\n",
    "      )\n",
    "    ],\n",
    "  )\n",
    "\n",
    "parsl.clear() # first clear the current configuration since we will likely run this script multiple times\n",
    "parsl.load(htex_config_local) # load the config we just outlined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_staging=1 # change this depending on data sample size!!!!!\n",
    "batch_size_rasterization=30\n",
    "batch_size_3dtiles=20\n",
    "batch_size_parent_3dtiles=500\n",
    "batch_size_geotiffs=200\n",
    "batch_size_web_tiles=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(items, batch_size):\n",
    "    \"\"\"\n",
    "    Create batches of a given size from a list of items.\n",
    "    \"\"\"\n",
    "    return [items[i:i + batch_size] for i in range(0, len(items), batch_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Staging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['/home/pdg/data/nitze_lake_change/data_sample_2022-09-09/32609/05_Lake_Dataset_Raster_02_final/lake_change.gpkg'],\n",
       " ['/home/pdg/data/nitze_lake_change/data_sample_2022-09-09/32608/05_Lake_Dataset_Raster_02_final/lake_change.gpkg'],\n",
       " ['/home/pdg/data/nitze_lake_change/data_sample_2022-09-09/32607/05_Lake_Dataset_Raster_02_final/lake_change.gpkg']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_batches = make_batch(data_paths, batch_size_staging)\n",
    "input_batches # 3 batches, 1 file each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage in parallel\n"
     ]
    }
   ],
   "source": [
    "# Decorators seem to be ignored as the first line of a cell, so print something first\n",
    "print(\"Stage in parallel\")\n",
    "\n",
    "@python_app\n",
    "def stage(paths, config, logging_dict = logging_dict): \n",
    "    \"\"\"\n",
    "    Stage files (step 1)\n",
    "    \"\"\"\n",
    "    import pdgstaging\n",
    "    if logging_dict:\n",
    "        import logging.config\n",
    "        logging.config.dictConfig(logging_dict)\n",
    "    stager = pdgstaging.TileStager(config)\n",
    "    for path in paths:\n",
    "        stager.stage(path)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [11], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m     app_future \u001b[39m=\u001b[39m stage(batch, workflow_config, logging_dict)\n\u001b[1;32m      4\u001b[0m     app_futures\u001b[39m.\u001b[39mappend(app_future)\n\u001b[0;32m----> 6\u001b[0m [a\u001b[39m.\u001b[39mresult() \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m app_futures]\n",
      "Cell \u001b[0;32mIn [11], line 6\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m     app_future \u001b[39m=\u001b[39m stage(batch, workflow_config, logging_dict)\n\u001b[1;32m      4\u001b[0m     app_futures\u001b[39m.\u001b[39mappend(app_future)\n\u001b[0;32m----> 6\u001b[0m [a\u001b[39m.\u001b[39;49mresult() \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m app_futures]\n",
      "File \u001b[0;32m~/anaconda3/envs/testing/lib/python3.9/concurrent/futures/_base.py:439\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    438\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[0;32m--> 439\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[1;32m    441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_condition\u001b[39m.\u001b[39mwait(timeout)\n\u001b[1;32m    443\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m~/anaconda3/envs/testing/lib/python3.9/concurrent/futures/_base.py:391\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[1;32m    390\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 391\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[1;32m    392\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    393\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    394\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/testing/lib/python3.9/site-packages/parsl/dataflow/dflow.py:288\u001b[0m, in \u001b[0;36mDataFlowKernel.handle_exec_update\u001b[0;34m(self, task_record, future)\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdone callback called, despite future not reporting itself as done\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    287\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 288\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_unwrap_remote_exception_wrapper(future)\n\u001b[1;32m    290\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    291\u001b[0m     logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mTask \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m try \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m failed\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(task_id, task_record[\u001b[39m'\u001b[39m\u001b[39mtry_id\u001b[39m\u001b[39m'\u001b[39m]))\n",
      "File \u001b[0;32m~/anaconda3/envs/testing/lib/python3.9/site-packages/parsl/dataflow/dflow.py:483\u001b[0m, in \u001b[0;36mDataFlowKernel._unwrap_remote_exception_wrapper\u001b[0;34m(future)\u001b[0m\n\u001b[1;32m    481\u001b[0m result \u001b[39m=\u001b[39m future\u001b[39m.\u001b[39mresult()\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(result, RemoteExceptionWrapper):\n\u001b[0;32m--> 483\u001b[0m     result\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m    484\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/testing/lib/python3.9/site-packages/parsl/app/errors.py:137\u001b[0m, in \u001b[0;36mRemoteExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mReraising exception of type \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(t))\n\u001b[1;32m    135\u001b[0m v \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_exception()\n\u001b[0;32m--> 137\u001b[0m reraise(t, v, v\u001b[39m.\u001b[39;49m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/testing/lib/python3.9/site-packages/six.py:719\u001b[0m, in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    717\u001b[0m     \u001b[39mif\u001b[39;00m value\u001b[39m.\u001b[39m__traceback__ \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m tb:\n\u001b[1;32m    718\u001b[0m         \u001b[39mraise\u001b[39;00m value\u001b[39m.\u001b[39mwith_traceback(tb)\n\u001b[0;32m--> 719\u001b[0m     \u001b[39mraise\u001b[39;00m value\n\u001b[1;32m    720\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    721\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/testing/lib/python3.9/site-packages/parsl/app/errors.py:175\u001b[0m, in \u001b[0;36mwrapper\u001b[0;34m()\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mparsl\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapp\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39merrors\u001b[39;00m \u001b[39mimport\u001b[39;00m RemoteExceptionWrapper\n\u001b[1;32m    174\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 175\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[39mreturn\u001b[39;00m RemoteExceptionWrapper(\u001b[39m*\u001b[39msys\u001b[39m.\u001b[39mexc_info())\n",
      "Cell \u001b[0;32mIn [10], line 15\u001b[0m, in \u001b[0;36mstage\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m stager \u001b[39m=\u001b[39m pdgstaging\u001b[39m.\u001b[39mTileStager(config)\n\u001b[1;32m     14\u001b[0m \u001b[39mfor\u001b[39;00m path \u001b[39min\u001b[39;00m paths:\n\u001b[0;32m---> 15\u001b[0m     stager\u001b[39m.\u001b[39mstage(path)\n\u001b[1;32m     16\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/testing/lib/python3.9/site-packages/pdgstaging/TileStager.py:132\u001b[0m, in \u001b[0;36mstage\u001b[0;34m()\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgrid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_tms_grid(gdf)\n\u001b[1;32m    131\u001b[0m     gdf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_properties(gdf, path)\n\u001b[0;32m--> 132\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_tiles(gdf)\n\u001b[1;32m    133\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    134\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mNo features in \u001b[39m\u001b[39m{\u001b[39;00mpath\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/testing/lib/python3.9/site-packages/pdgstaging/TileStager.py:426\u001b[0m, in \u001b[0;36msave_tiles\u001b[0;34m()\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[39m# Record what was saved\u001b[39;00m\n\u001b[1;32m    425\u001b[0m     data[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprops[\u001b[39m'\u001b[39m\u001b[39mtile\u001b[39m\u001b[39m'\u001b[39m]] \u001b[39m=\u001b[39m tiles\n\u001b[0;32m--> 426\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msummarize(data)\n\u001b[1;32m    427\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    428\u001b[0m     \u001b[39m# Track the end time, the total time, and the number of vectors\u001b[39;00m\n\u001b[1;32m    429\u001b[0m     logger\u001b[39m.\u001b[39minfo(\n\u001b[1;32m    430\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mSaved \u001b[39m\u001b[39m{\u001b[39;00mtile_path\u001b[39m}\u001b[39;00m\u001b[39m in \u001b[39m\u001b[39m{\u001b[39;00mdatetime\u001b[39m.\u001b[39mnow() \u001b[39m-\u001b[39m start_time\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m    431\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/testing/lib/python3.9/site-packages/pdgstaging/TileStager.py:562\u001b[0m, in \u001b[0;36msummarize\u001b[0;34m()\u001b[0m\n\u001b[1;32m    555\u001b[0m gdf_summary\u001b[39m.\u001b[39mto_csv(summary_path, mode\u001b[39m=\u001b[39mmode,\n\u001b[1;32m    556\u001b[0m                    index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, header\u001b[39m=\u001b[39mheader)\n\u001b[1;32m    558\u001b[0m \u001b[39m# Log the total time to create the summary\u001b[39;00m\n\u001b[1;32m    559\u001b[0m logger\u001b[39m.\u001b[39minfo(\n\u001b[1;32m    560\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mSummarized Tile(z=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, x=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, y=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) in \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m    561\u001b[0m     \u001b[39m.\u001b[39mformat(\n\u001b[0;32m--> 562\u001b[0m         tile_props[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mtile_z\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m    563\u001b[0m         tile_props[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mtile_x\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m    564\u001b[0m         tile_props[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mtile_y\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m    565\u001b[0m         (datetime\u001b[39m.\u001b[39mnow() \u001b[39m-\u001b[39m start_time)\n\u001b[1;32m    566\u001b[0m     )\n\u001b[1;32m    567\u001b[0m )\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "app_futures = []\n",
    "for batch in input_batches:\n",
    "    app_future = stage(batch, workflow_config, logging_dict)\n",
    "    app_futures.append(app_future)\n",
    "\n",
    "[a.result() for a in app_futures]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rasterization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get paths to all the newly staged tiles\n",
    "staged_paths = stager.tiles.get_filenames_from_dir('staged')\n",
    "staged_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(staged_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch staged files\n",
    "staged_batches = make_batch(staged_paths, batch_size_rasterization)\n",
    "len(staged_batches) # 634 batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see what is within 1 batch\n",
    "staged_batches[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('rasterize in parallel')\n",
    "\n",
    "@python_app\n",
    "def rasterize(staged_paths, config, logging_dict = logging_dict):\n",
    "    \"\"\"\n",
    "    Rasterize a batch of vector files (step 2)\n",
    "    \"\"\"\n",
    "    import pdgraster\n",
    "    if logging_dict:\n",
    "        import logging.config\n",
    "        logging.config.dictConfig(logging_dict)\n",
    "    rasterizer = pdgraster.RasterTiler(config)\n",
    "    return rasterizer.rasterize_vectors(staged_paths, make_parents = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_futures = []\n",
    "for batch in staged_batches:\n",
    "    app_future = rasterize(batch, workflow_config, logging_dict)\n",
    "    app_futures.append(app_future)\n",
    "\n",
    "# Don't continue to step 3 until all tiles have been rasterized\n",
    "[a.result() for a in app_futures]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the following:\n",
    "\n",
    "1. number of rasters in z-level 11 compared to number of staged files (which are all z-level 11)\n",
    "2. number of rasters in all z-levels matches number of rasters in all z-levels from other run-through not in parallel (lake_change_sample dir)\n",
    "3. any weird formatting in rasters_summary.csv? fix if so\n",
    "4. any errors reported in rasterization_events.csv?\n",
    "4. any errors reported in log.log?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Web Tiles from geoTIFF's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update ranges from raster_summary.csv\n",
    "rasterizer.update_ranges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process web tiles in batches\n",
    "geotiff_paths = tile_manager.get_filenames_from_dir('geotiff')\n",
    "geotiff_batches = make_batch(geotiff_paths, batch_size_web_tiles)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('testing': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d5d1d9449773c8f18500bd0744116e15984b8edc50fb3891307276385a43395f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
